# v4 Quantum Phase-Field LLM

A novel language model architecture combining quantum-inspired phase representations with GPU-practical implementations.

## Key Innovation

Unlike traditional transformers or even v2/v3, v4 uses:

- **Phase2D Representation**: Complex numbers as 2D real vectors (no sin/cos in hot path)
- **Morphological Tokenization**: Words split into Root + Affix, where Affix applies a phase rotation (tense/aspect) to the Root (meaning).
- **Multi-Layer Phase Banks**: Separate semantic/context/language/emotion layers that interfere
- **Oscillatory SSM Backbone**: Linear-time sequence processing via coupled oscillators
- **Phase-Coded Memory**: Long-term associative memory with coherence-based retrieval
- **Injectable Architecture**: All components swappable via registry/config

## Quick Start

```bash
cd v4

# Run tests to validate everything works
uv run python test_v4.py

# Train on random data (for testing architecture)
uv run python train.py --size tiny --epochs 2

# Train on REAL data (WikiText-2)
uv run python train_real.py --dataset wikitext2 --size small --epochs 5

# Train on TinyStories (good for small models)
uv run python train_real.py --dataset tinystories --size small --epochs 10 --max_train_samples 5000
```

## Architecture Overview

```
Tokens â†’ Phase2D Embed â†’ Phase Banks â†’ Backbone â†’ Memory â†’ Coupler â†’ LM Head
                           â†“
            [Semantic, Context, Language, Emotion]
                           â†“
                   Interference Coupling
```

### Components

| Component | Description | Implementation |
|-----------|-------------|----------------|
| **Phase2D** | Complex numbers as [real, imag] pairs | `core/phase2d.py` |
| **PhaseBank** | Separate meaning layers | `banks/` |
| **Backbone** | Oscillatory SSM | `backbone/oscillatory_ssm.py` |
| **Coupler** | Interference-based mixing | `coupler/interference.py` |
| **Memory** | Phase-coded associative memory | `memory/phase_associative.py` |
| **Objectives** | CE + coherence + energy losses | `objectives/` |
| **Sampler** | Autoregressive sampling | `sampler/autoregressive.py` |

## Phase2D: The Core Math

Instead of using sin/cos for phase operations (slow on GPU), we represent complex numbers as 2D vectors:

```python
# Complex number z = a + bi represented as
z = torch.tensor([a, b])  # shape: [..., 2]

# Multiplication by i (90Â° rotation)
i * z = torch.tensor([-b, a])  # Just swap and negate!

# Complex multiplication (a + bi) * (c + di)
result_real = a*c - b*d
result_imag = a*d + b*c
```

All operations reduce to matrix multiplies (GEMM) - perfect for Tensor Cores.

## Injectable Architecture

Every component can be swapped via config:

```python
from v4.core.config import V4Config, BankConfig

config = V4Config(
    dim=256,
    banks={
        'semantic': BankConfig(type='semantic', dim=256),
        'context': BankConfig(type='context', dim=256),
        'my_custom': BankConfig(type='my_custom_bank', dim=256),  # Your bank!
    },
)
```

Register new components with decorators:

```python
from v4.core.registry import register_bank

@register_bank('my_custom_bank', description='My custom phase bank')
class MyCustomBank(PhaseBank):
    ...
```

## Model Sizes

| Size | Dim | Layers | Params | Use Case |
|------|-----|--------|--------|----------|
| tiny | 64 | 4 | ~1M | Testing |
| small | 256 | 8 | ~10M | Quick experiments |
| medium | 512 | 12 | ~50M | Balanced |
| large | 768 | 16 | ~200M | Production |

## Training

### With Real Data (Recommended)

```bash
# WikiText-2 (quick validation)
uv run python train_real.py --dataset wikitext2 --size small --epochs 10

# TinyStories (better for small models)
uv run python train_real.py --dataset tinystories --size small --epochs 20

# Medium model
uv run python train_real.py --dataset tinystories --size medium --epochs 20 --batch_size 4

# Resume training
uv run python train_real.py --dataset tinystories --size small --resume checkpoints_v4_real/best_model.pt
```

### With Random Data (Architecture Testing)

```bash
uv run python train.py --size tiny --epochs 5
```

## File Structure

```
v4/
â”œâ”€â”€ core/                    # Core abstractions
â”‚   â”œâ”€â”€ phase2d.py          # Phase2D math (the foundation)
â”‚   â”œâ”€â”€ interfaces.py       # Base classes (PhaseBank, Backbone, etc.)
â”‚   â”œâ”€â”€ registry.py         # Factory pattern for components
â”‚   â””â”€â”€ config.py           # Configuration system
â”œâ”€â”€ banks/                   # Phase bank implementations
â”‚   â”œâ”€â”€ semantic.py         # Semantic meaning layer
â”‚   â”œâ”€â”€ context.py          # Context/syntax layer
â”‚   â””â”€â”€ language.py         # Language-specific + emotion layers
â”œâ”€â”€ backbone/               # Sequence backbone
â”‚   â””â”€â”€ oscillatory_ssm.py  # Oscillatory state-space model
â”œâ”€â”€ coupler/                # Bank coupling
â”‚   â””â”€â”€ interference.py     # Interference-based coupling
â”œâ”€â”€ memory/                 # Long-term memory
â”‚   â””â”€â”€ phase_associative.py # Phase-coded associative memory
â”œâ”€â”€ objectives/             # Loss functions
â”‚   â”œâ”€â”€ ce.py              # Cross-entropy
â”‚   â””â”€â”€ coherence.py       # Coherence + energy losses
â”œâ”€â”€ sampler/               # Generation strategies
â”‚   â””â”€â”€ autoregressive.py  # AR sampling
â”œâ”€â”€ data/                   # Dataset integration
â”‚   â”œâ”€â”€ datasets.py        # WikiText-2, TinyStories, etc.
â”‚   â””â”€â”€ tokenizer.py       # GPT-2 tokenizer wrapper
â”œâ”€â”€ model.py               # Main model (wires everything)
â”œâ”€â”€ train.py               # Training (random data, for testing)
â”œâ”€â”€ train_real.py          # Training with real datasets
â””â”€â”€ test_v4.py             # Test suite
```

## Comparison with v2/v3

| Feature | v2 | v3 | v4 |
|---------|----|----|-----|
| Phase representation | sin/cos | N/A | Phase2D (no trig) |
| Separate meaning layers | Partial | N/A | Full (banks) |
| Sequence complexity | O(nÂ²) | O(nÂ²) | O(n) linear |
| Long context | Limited | Limited | 256K target |
| Incremental learning | No | Partial | Full (shards) |
| GPU efficiency | Medium | Medium | High (GEMM-only) |

## Next Steps

1.  **Morphological Tokenizer**: Implement custom tokenizer that splits words into `(root, affix)` pairs.
    *   *Idea*: "walking" â†’ `root="walk"`, `affix="ing"`.
    *   *Mechanism*: Root sets the base phase vector, Affix applies a rotation (IotaBlock) to modify tense/aspect.
2.  **Dataset Integration**: Connect to v3's dataset system
3.  **256K Context**: Implement chunked processing + state management
4.  **Custom Kernels**: Triton kernels for Phase2D ops
5.  **Benchmarking**: Compare with v2/v3 on perplexity/speed

## Status

**v4 is in active development.**

- âœ… Core Phase2D math (no trig in hot path)
- âœ… All interfaces defined (PhaseBank, Coupler, Backbone, Memory, Objectives, Sampler)
- âœ… First implementations of each component
- âœ… Injectable architecture (registry + config)
- âœ… Model wiring
- âœ… Basic training loop
- âœ… Test suite (all tests pass)
- âœ… Real dataset integration (WikiText-2, TinyStories)
- âœ… GPT-2 tokenizer integration
- ðŸ”„ Validate training (run on real data, check perplexity drops)
- ðŸ”„ Incremental learning test (memory sharding)
- ðŸ”„ Long context support (256K streaming)
- ðŸ”„ Custom CUDA/Triton kernels
- ðŸ”„ Landmark measurement module
